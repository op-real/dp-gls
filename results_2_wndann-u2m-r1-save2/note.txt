weight learning:
batch_size: 64
epoch: 1


if epoch > start_epoch and method != 'NANN':
    optimizer_ad.step()

        # learn/update weights for WNDANN
if 'WN' in method and epoch > start_epoch:
    train_wn(args, model, source_samples, target_samples, wn_disc, wn_gen, optimizer_wnd, optimizer_wng, nn.BCEWithLogitsLoss(), nn.BCEWithLogitsLoss(reduction='none'))



class WNGen(nn.Module):
  def __init__(self, in_feature, hidden_size):
    super(WNGen, self).__init__()
    self.ad_layer1 = nn.Linear(in_feature, hidden_size)
    self.ad_layer2 = nn.Linear(hidden_size, hidden_size)
    self.ad_layer3 = nn.Linear(hidden_size, 1)
    self.relu1 = nn.ReLU()
    self.relu2 = nn.ReLU()
    self.dropout1 = nn.Dropout(0.5)
    self.dropout2 = nn.Dropout(0.5)
    self.softmax = nn.Softmax(dim=0)
    self.sigmoid = nn.Sigmoid()
    self.leakyRelu = nn.LeakyReLU(0)
    self.apply(init_weights)

  def forward(self, x):
    x = x * 1.0
    x = self.ad_layer1(x)
    x = self.relu1(x)
    x = self.dropout1(x)
    x = self.ad_layer2(x)
    x = self.relu2(x)
    x = self.dropout2(x)
    y = self.ad_layer3(x)
    y = self.sigmoid(y)
    y = torch.clamp(y, min=0.1)
    y = y.reshape(-1)

    return y